{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Project Proposal\n",
    "\n",
    "## Authors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Daniel Galicia Ortiz - Background Research\n",
    "- Jordan Chang - Data\n",
    "- Yiou Huang - Hypothesis, Background Research, Writing - Original Proposal\n",
    "- Wenxin Miao - Data\n",
    "- Johnson Chung - Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To what extent does (the user’s attitude towards AI: another individual VS. a tool) the use of anthropomorphic versus instrumental language in app store reviews predict user satisfaction with popular AI models (including ChatGPT, Gemini, and Claude), as measured by star ratings?\n",
    "\n",
    "In this study, we operationalize anthropomorphic language as textual expressions that frame an AI system as possessing human-like qualities—including mental states (e.g., thinking, understanding), emotions (e.g., frustration, excitement), social abilities (e.g., listening, communicating), agency (e.g., deciding, choosing), or personhood (e.g., identity, vulnerability)—thereby treating the AI as another individual rather than as a functional tool (Source 1). Conversely, instrumental language is defined as language that frames the AI system in purely functional or technical terms, emphasizing its utility, performance characteristics, and machine-like properties without attributing human characteristics (e.g., "the algorithm processes requests efficiently," "the software generates accurate outputs"). User attitudes toward AI—whether they perceive it as another individual or as a tool—are expressed through these linguistic choices: when users anthropomorphize, their language reveals an implicit mental model of the AI as a social actor deserving of human-like treatment, whereas instrumental language reflects a mental model of the AI as an object to be evaluated solely on functional performance (Source 2; Source 3).\n",
    "\n",
    "We employ a two-stage measurement approach that combines automated computational scoring with manual validation to ensure both scalability and accuracy. In the first stage, we apply AnthroScore (Source 4), a computational linguistic measure that quantifies the degree to which a non-human entity is implicitly framed as human-like by the surrounding linguistic context. AnthroScore uses a masked language model (RoBERTa) to compute, for each mention of the AI system in a review (e.g., "ChatGPT," "the app"), the probability that the masked entity would be human versus non-human based on contextual cues; the resulting continuous score indicates whether the entity is framed anthropomorphically (positive scores), instrumentally (negative scores), or neutrally (scores near zero) (Source 4). This automated approach allows us to process our full dataset of app store reviews efficiently while providing a validated, lexicon-free metric that captures implicit anthropomorphism beyond simple keyword matching.\n",
    "\n",
    "In the second stage, we validate and refine our classifications through manual coding using the taxonomy developed by DeVrio et al. (2025), which identifies 19 distinct types of linguistic expressions that contribute to anthropomorphism of language technologies. This taxonomy, derived from empirical analysis of user interactions with AI systems and organized across five conceptual dimensions—cognitive abilities (e.g., "understands," "learns"), social abilities (e.g., "listens," "communicates"), feelings and desires (e.g., "gets frustrated," "wants to help"), physical actions and embodiment (e.g., "looked at," "grabbed"), and identity and personhood (e.g., "my friend," "has a personality")—provides a granular framework for identifying specific anthropomorphic expressions that may appear in app reviews (Source 1). We randomly sample 10% of reviews stratified by AnthroScore levels (low, medium, high) and manually code them for the presence of anthropomorphic expression types, allowing us to verify that computationally-flagged reviews do indeed contain anthropomorphic language and to identify any systematic patterns the automated scoring may miss.\n",
    "\n",
    "Based on the combined evidence from AnthroScore and manual coding, we classify each review into one of three categories: anthropomorphic (high AnthroScore and/or presence of two or more expression types from the DeVrio et al. taxonomy), instrumental (low or negative AnthroScore, absence of anthropomorphic expressions, and presence of technical/functional language), or neutral/mixed (ambiguous cases, which we exclude from the primary analysis). This triangulated approach provides robust construct validity by leveraging the scalability of computational methods alongside the nuance and verification that manual coding affords, ensuring that our independent variable accurately captures the distinction between treating AI as an individual versus treating it as a tool.\n",
    "\n",
    "**Reference**\n",
    "1. DeVrio, A., Cheng, M., Egede, L., Olteanu, A., & Blodgett, S. L. (2025). A taxonomy of linguistic expressions that contribute to anthropomorphism of language technologies. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI '25). https://doi.org/10.1145/3706598.3714038\n",
    "2. Epley, N., Waytz, A., & Cacioppo, J. T. (2007). On seeing human: A three-factor theory of anthropomorphism. Psychological Review, 114(4), 864–886. https://doi.org/10.1037/0033-295X.114.4.864\n",
    "3. Nass, C., Steuer, J., & Tauber, E. R. (1994). Computers are social actors. In Proceedings of the CHI '94 Conference on Human Factors in Computing Systems (pp. 418–423). https://doi.org/10.1145/191666.191703\n",
    "4. Cheng, M., Gligorić, K., Piccardi, T., & Jurafsky, D. (2024). AnthroScore: A computational linguistic measure of anthropomorphism. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024), Vol. 1: Long Papers (pp. 807–825). https://doi.org/10.18653/v1/2024.eacl-long.49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is no surprise that AI has a profound impact on our lives. Before the 2010s, AI largely belonged to the realm of fiction, imagined as computers capable of humanlike conversation, reasoning and sometimes human emotion. In recent years, however, this fiction has become reality with the development of advanced models such as ChatGPT and Gemini, with roughly about 41% of Americans saying that they use these or similar tools. (Source 2) These large language models (LLMs) make AI interactions feel more human, enabling deeper integration into daily life. At the same time, a growing concern about AI has slowly grown over the last year, with topics ranging from jobs and privacy, to the decline of social interaction. (Source 1) Together, these examples highlight the wide range of human attitudes toward AI and raise an important question: can we predict how people perceive an AI tool based on how they treat and interact with it? This is a question that we feel the need to find some answer to.\n",
    "\n",
    "As noted earlier, AI is no longer a niche technology. It has become increasingly normalized in society, particularly among younger generations. For example, roughly two-thirds of teenagers in the United States report having used an AI chatbot (Source 3). This raises a natural follow-up question: how do people tend to perceive AI? The answer is somewhat surprising. It is becoming more common for individuals to develop personal or relational attitudes toward AI chatbots. One study found that 38% of users believe that large language models will eventually form “deep relationships with humans” (Source 4). Other research has also suggested a growing association between AI and emotionally meaningful human interaction. Anthropomorphism, attributing human characteristics to AI, has been shown to increase users’ willingness to adopt AI services (Source 5). Although that study emphasized the need for further testing, it still identified a positive relationship between anthropomorphism and user engagement. Together, these findings help lay the groundwork for the question we aim to investigate.\n",
    "\n",
    "Unfortunately no study is without limitations, and by examining prior research and studies we can identify weaknesses in data collection and methodology. For instance, some studies rely heavily on participants’ beliefs and self-reports, which can be influenced by popular culture and/or media narratives, limited technical understanding, and even the wording of survey questions. Framing effects—such as using terms like “erosion,” “loss,” or “control”—can shape how respondents interpret and answer questions (Source 1). Other studies face issues related to human estimation and omission. When research depends on approximations, results may overestimate or underestimate reality. Additionally, some studies do not distinguish how AI is used across contexts, such as for work, personal, or emotional purposes (Source 3). Although Source 3 draws on a nationally representative, demographically weighted sample, this does not fully address contextual gaps in AI usage. Because our focus is on measuring anthropomorphic language, no single dataset will be perfect; however, combining multiple datasets can help mitigate these limitations and produce more balanced insights.\n",
    "\n",
    "While the research and articles above analyze different aspects of the issue—such as public views of AI or patterns of AI usage—we believe there is a need for a complementary approach to determine whether a relationship between our two factors truly exists. To properly identify such a relationship, it is important to consider multiple external variables to ensure that any observed connection is meaningful rather than coincidental. In collecting data, particularly from app reviews, we plan to screen for anthropomorphic language used to describe or address AI systems. We also propose that several contextual factors may influence this language. These include AI usage (for example, whether AI is used for work, advice, or emotional support), AI purpose (since different systems are designed for general assistance versus specialized chatbot functions), and AI customization (such as response style, tone, or added voice features, including perceived gender). Each of these elements may shape how users conceptualize and describe AI. By integrating these dimensions, we aim to better understand whether there is a relationship between how people perceive AI and the extent to which they talk about it in human-like versus machine-like terms(he/she/them vs it).\n",
    "\n",
    "**References**\n",
    "1. https://www.pewresearch.org/science/2025/09/17/how-americans-view-ai-and-its-impact-on-people-and-society/  \n",
    "2. https://techequity.us/2025/10/07/how-people-really-feel-about-ai-from-sea-to-shining-se/   \n",
    "3. https://www.pewresearch.org/wp-content/uploads/sites/20/2025/12/PI_2025.12.09_Teens-Social-Media-AI_REPORT.pdf  \n",
    "4. https://imaginingthedigitalfuture.org/reports-and-publications/close-encounters-of-the-ai-kind/\n",
    "5. https://link.springer.com/article/10.1007/s11747-020-00762-y#Sec12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis (H₀): There is no significant association between the degree of anthropomorphic language used in app store reviews of AI systems and the star ratings given by users. In other words, whether a reviewer frames an AI system as a human-like entity or as a purely functional tool does not predict their reported level of satisfaction.\n",
    "\n",
    "Alternative Hypothesis (H₁): There is a significant positive association between the degree of anthropomorphic language used in app store reviews of AI systems and the star ratings given by users. Specifically, reviews that frame AI systems using more anthropomorphic language (i.e., treating AI as another individual) will be associated with higher star ratings compared to reviews that use predominantly instrumental language (i.e., treating AI as a tool).\n",
    "\n",
    "We predict a significant positive association between anthropomorphic language and higher star ratings for three converging reasons drawn from the background literature. First, Blut et al. (2021) (Source 6) synthesized 108 studies involving over 11,000 individuals and found that customer anthropomorphism of AI systems, including chatbots, functions as a positive mediator toward user intention and satisfaction, operating through increased perceptions of social presence and trust. This suggests that users who psychologically engage with AI as a social partner rather than a mechanical instrument are likely to report more favorable experiences. Second, Cheng et al. (2025) (Source 7) demonstrated using over 12,000 nationally representative responses that perceptions of AI's human-likeness significantly predict both trust and willingness to adopt AI (r² = 0.21, p < 0.001), and that these anthropomorphic perceptions increased by 34% over a single year, indicating that the trend toward viewing AI as more human-like is becoming more and more prevalent. Third, Epley, Waytz, and Cacioppo (2007) (Source 8) established that anthropomorphism is driven in part by sociality motivation: a deep human need to connect socially, which implies that users who express anthropomorphic language about an AI app are likely doing so because the interaction fulfilled a social need, an experience that would naturally translate into greater satisfaction and, consequently, higher star ratings. Furthermore, we will explore the relationships between multiple dimensions (AI usage, AI purpose, and AI customization) and people’s attitude towards AI as well as their satisfaction level.\n",
    "\n",
    "Although the weight of evidence supports our directional prediction, it is important to acknowledge that the relationship between anthropomorphic language and satisfaction may not be significant. Crolic et al. (2022) (Source 9) found across five studies, including a large real-world telecommunications dataset, that chatbot anthropomorphism actually produced a negative effect on customer satisfaction when users entered the interaction in an angry emotional state, because anthropomorphic framing inflated expectations of the chatbot's capabilities and set the stage for expectancy violations when those expectations went unmet, a dynamic that could similarly contaminate app store reviews, where frustrated users are disproportionately likely to write reviews in the first place. Furthermore, Blut et al. (2021) (Source 6) noted in their meta-analysis that the positive effects of anthropomorphism on satisfaction were heavily moderated by context and task type, meaning that in utilitarian, task-completion-oriented interactions, arguably the dominant use case for most AI apps, instrumental framing may matter as much or more than anthropomorphic framing for predicting user satisfaction.\n",
    "\n",
    "**References**\n",
    "6. Blut, M., Wang, C., Wünderlich, N. V., & Brock, C. (2021). Understanding anthropomorphism in service provision: A meta-analysis of physical robots, chatbots, and other AI. Journal of the Academy of Marketing Science, 49(4), 632–658. https://doi.org/10.1007/s11747-020-00762-y  \n",
    "7. Cheng, M., Lee, A. Y., Rapuano, K., Niederhoffer, K., Liebscher, A., & Hancock, J. T. (2025). From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors. In Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT '25). https://arxiv.org/abs/2501.18045   \n",
    "8. Epley, N., Waytz, A., & Cacioppo, J. T. (2007). On seeing human: A three-factor theory of anthropomorphism. Psychological Review, 114(4), 864–886. https://doi.org/10.1037/0033-295X.114.4.864  \n",
    "9. Crolic, C., Thomaz, F., Hadi, R., & Stephen, A. T. (2022). Blame the bot: Anthropomorphism and anger in customer–chatbot interactions. Journal of Marketing, 86(1), 132–148. https://doi.org/10.1177/00222429211045687 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. The **ideal** dataset \n",
    "   1. Variables: The IV is the user's attitude towards AI (another individual vs. a tool) operationalized as the use of anthropomorphic versus instrumental language in app store reviews. The DV are the user's ratings (i.e. scores) to Gen AI models. \n",
    "   2. How many observations needed: Our study focuses relies on extracting specific reviews that mention anthropomorphic or utilitarian keywords, however many comments are non-substantive (e.g. 'Nice'). In order to ensure we retain sufficient statistical power after data cleaning and keyword filtering, we plan to scrape about 5,000 raw reviews per app (ChatGPT, Gemini, Claude), resulting in a total raw dataset of roughly 15,000 observations. This sample size provides a safety margin to account for the expected data loss dring the data wrangling. \n",
    "   3. How the data will be collected: All of the app reviews are directly scraped from the Google Play Store by our group members, referring to the scraping method published by Kaggle user Ashish Humar. The dataset includes the variables and also the review time, username, id, thumb-up count, and version. Username and id will be cleaned due to ethics, privacy, and useless, while others may be considered as weighting factor or control variables. \n",
    "   4. How the data will be stored and organized: The scraped data will be stored in CSV files and stored in our Github repository. The too short or non-substantial review content and the personality identifiable information (username and id) will be dropped. The cleaned three datasets will be merged into one, but the name of the apps will be kept for data analysis and EDA. \n",
    "2. Potential **real** datasets \n",
    "   1. Dataset 1: Google Play Store reviews of ChatGPT\n",
    "      - [Scraping Method Reference](https://www.kaggle.com/code/ashishkumarak/chatgpt-google-play-reviews-scraping/notebook)\n",
    "      - Source: self-scrape\n",
    "      - We will use the 'content' column from which we will extract words and decide the anthropomorphic level. And we will use the 'score' column as a proxy of user satisfaction to analyze the potential correlation between the anthropomorphic level and user satisfaction. \n",
    "   3. Dataset 2: Google Play Store reviews of Gemini \n",
    "      - Source: self-scrape \n",
    "      - We will use the review content and rating to see whether users like to give positive or negative scores for anthropomorphic or industrial features. This dataset is for covering more AI models. \n",
    "   4. Dataset 3: Google Play Store reviews of Claude \n",
    "      - Source: self-scrape \n",
    "      - We will use the review content and rating to see whether users like to give positive or negative scores for anthropomorphic or industrial features. This dataset is for covering more AI models. \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics \n",
    "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "> The project uses publicly available app reviews that were written voluntarily by the users. The reviewers did not provide informed consent for their review to be used in research, but the data was collected publically from the Google Play store with no private information being collected. In those places you can only see a user’s username. The people who got their reviews scrapped may not have posted it if they knew that it would be used as data for our research question, so we will be analyzing the text of the review without any private information.\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "> We recognize that a user who leaves reviews may not be the same as a person who doesn’t leave reviews. People who leave reviews are likely to report extreme positive or negative experiences, so their language may not reflect the population.\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "> User data such as usernames will be cleaned to not show any personal information.\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "> We are not collecting protected attributes. That information is not displayed and even if it was available it was not going to be used because it is not a part of our analysis.\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "> The security is not too advanced but the data will be stored on a private GitHub repo with our group members and the TAs being able to access it.\n",
    " - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "> \n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "> The analysis only focuses on users who choose to leave written reviews, which may exclude the perspective of users who have downloaded and use the app but haven’t left a review. Also during our meetings, we have decided to scrape only Google Play reviews and not App Store anymore due to the App Store being more difficult, meaning Apple users may be excluded.\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "> The text of the reviews may be biased by cultural and linguistic differences in how people talk. Even if we take only English reviews, there are many different places that speak English differently in how they speak and text. For example, some cultures may use “thank you” more often out of habit rather than treating the AI like a human.\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    "> We are not analyzing data with PPI. It is just the raw text of their reviews. This means that no private information is being used, and therefore the analysis will be in privacy.\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "> The analysis is limited to reviews written in English because the sentiment analysis is meant for English and translating reviews from other languages can cause the meaning of the words to change. This may exclude years from non-English-speaking regions and therefore indirectly limit representation of certain cultural or geographical groups. As a result, the findings may not generalize to all users.\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    "> Our analysis uses a model from GitHub to classify text as \"anthropomorphic\" or “instrumental”. To ensure explainability, we have reviewed the repository’s documentation and tried to understand how it works. We can justify the model’s decision making by looking at some of the reviews and comparing the score to what we would expect to see if the logic aligns with how we think.\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "> The way the score of the user reviews is inferred from the language algorithm and is not a definitive measure of a users’ belief. The analysis is correlational and limited to English reviews from the platform and the AI app. Therefore, the results may not generalize to all people who use AI.\n",
    "\n",
    "### E. Deployment\n",
    " - [ ] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [ ] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [ ] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Team Expectation 1*: Do not show up late for meetings\n",
    "* *Team Expectation 2*: Complete your section of the work by the deadline of the week\n",
    "* *Team Expecation 3*: If you cannot make it or will be very late, give a heads up\n",
    "* *Team Expecation 4*: Be respectful to each other\n",
    "* *Team Expecation 5*: Attend the team meeting every week\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "| Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/2  |  3 PM | Determine the RQ, do background research on the topic  | Research potential tasets to use; work on research proposal | \n",
    "| 2/4  |  N/A |  Project Proposal | N/A | \n",
    "| 2/9  | 3 PM  | Import and wrangling data  | Discuss data analysis plan; address TA feedback on the project proposal   |\n",
    "| 2/16  | 3 PM  | Finalized data wrangling | Discuss and analyze the data   |\n",
    "| 2/18  | N/A  | Data Checkpoint | N/A |\n",
    "| 2/23  | 3 PM  | N/A | Address feedback from TA and start talking about analysis |\n",
    "| 3/2  | 3 PM  | Complete Analysis | Discuss/draft reselts, discussion, conclusion |\n",
    "| 3/4  | N/A  | EDA Checkpoint | N/A |\n",
    "| 3/9  | 3 PM  | Edit full project | Refine final project |\n",
    "| 3/18  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
